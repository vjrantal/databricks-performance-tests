{"cells":[{"cell_type":"code","source":["# Before running, either set the matching secrets (https://docs.azuredatabricks.net/user-guide/secrets/secrets.html)\n# or edit the variables below to contain valid connection details\n\nsecrets_scope = \"KEYS\"\n\ndata_lake_app_id = dbutils.secrets.get(secrets_scope, \"DATA_LAKE_APP_ID\")\ndata_lake_app_key = dbutils.secrets.get(secrets_scope, \"DATA_LAKE_APP_KEY\")\ndata_lake_app_tenant = dbutils.secrets.get(secrets_scope, \"DATA_LAKE_APP_TENANT\")\ndata_lake_account = dbutils.secrets.get(secrets_scope, \"DATA_LAKE_ACCOUNT\")\n\nstorage_account_key = dbutils.secrets.get(secrets_scope, \"STORAGE_ACCOUNT_KEY\")\nstorage_account = dbutils.secrets.get(secrets_scope, \"STORAGE_ACCOUNT\")\n\nmount_folder = \"dummy\"\ndata_lake_mount_point = \"/mnt/lake\"\nstorage_mount_point = \"/mnt/blob\"\n\n# Data Lake connectioction information and credentials\ndata_lake_configs = {\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n           \"dfs.adls.oauth2.client.id\": data_lake_app_id,\n           \"dfs.adls.oauth2.credential\": data_lake_app_key,\n           \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com/%s/oauth2/token\" % data_lake_app_tenant}\n\ndbutils.fs.mount(\n  source = \"adl://%s.azuredatalakestore.net/%s\" % (data_lake_account, mount_folder),\n  mount_point = data_lake_mount_point,\n  extra_configs = data_lake_configs)\n\nstorage_configs = {\"fs.azure.account.key.%s.blob.core.windows.net\" % storage_account: storage_account_key}\n\ndbutils.fs.mount(\n  source = \"wasbs://%s@%s.blob.core.windows.net/%s\" % (mount_folder, storage_account, mount_folder),\n  mount_point = storage_mount_point,\n  extra_configs = storage_configs)\n\ndbutils.fs.mounts()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from random import random\nfrom time import time\nimport math\nimport os\n\nimport avro.schema\nfrom avro.datafile import DataFileWriter\nfrom avro.io import DatumWriter\n\nfirst_folder_count = 10\nsecond_folder_count = 10\nthird_folder_count = 10\nrows_per_file = 10\n\nschema_string = '''\n{\n  \"type\" : \"record\",\n  \"name\" : \"Message\",\n  \"namespace\" : \"Microsoft.Azure.Devices\",\n  \"fields\" : [ {\n    \"name\" : \"Body\",\n    \"type\" : [ \"null\", \"bytes\" ]\n  } ]\n}\n'''\nschema = avro.schema.parse(schema_string)\n\nprint('Amount of rows in each file: %d' % rows_per_file)\nprint('Total amount of files: %d' % (first_folder_count * second_folder_count * third_folder_count))\n\ntry:\n  dbutils\nexcept NameError:\n  mkdirs = os.makedirs\nelse:\n  mkdirs = dbutils.fs.mkdirs\n\ndef create_file(*args):\n  folder_path = os.path.join(*map(str, args))\n  mkdirs(folder_path)\n  file_name = \"dummy.avro\"\n  file_path = os.path.join(folder_path, file_name)\n  writer = DataFileWriter(open(file_path, \"wb\"), DatumWriter(), schema)\n  for i in range(rows_per_file):\n    writer.append({\"Body\": \"{\\\"id\\\":\\\"sensor-id-%s\\\",\\\"v\\\":%f,\\\"t\\\":%d}\" % (i, random(), time() * 1000)})\n  writer.close()\n\ndef create_files(root_folder):\n  for first in range(first_folder_count):\n    for second in range(second_folder_count):\n      for third in range(third_folder_count):\n        create_file(root_folder, first, second, third)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["create_files(storage_mount_point)\ndbutils.fs.ls(storage_mount_point)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["create_files(data_lake_mount_point)\ndbutils.fs.ls(data_lake_mount_point)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["dbutils.fs.unmount(data_lake_mount_point)\ndbutils.fs.unmount(storage_mount_point)"],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"GeneratingTestData","notebookId":590758760657087},"nbformat":4,"nbformat_minor":0}
