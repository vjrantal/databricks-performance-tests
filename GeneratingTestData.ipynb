{"cells":[{"cell_type":"code","source":["# Before running, either set the matching secrets (https://docs.azuredatabricks.net/user-guide/secrets/secrets.html)\n# or edit the variables below to contain valid connection details\n\nsecrets_scope = \"KEYS\"\n\ndata_lake_app_id = dbutils.secrets.get(secrets_scope, \"DATA_LAKE_APP_ID\")\ndata_lake_app_key = dbutils.secrets.get(secrets_scope, \"DATA_LAKE_APP_KEY\")\ndata_lake_app_tenant = dbutils.secrets.get(secrets_scope, \"DATA_LAKE_APP_TENANT\")\ndata_lake_account = dbutils.secrets.get(secrets_scope, \"DATA_LAKE_ACCOUNT\")\n\nstorage_account_key = dbutils.secrets.get(secrets_scope, \"STORAGE_ACCOUNT_KEY\")\nstorage_account = dbutils.secrets.get(secrets_scope, \"STORAGE_ACCOUNT\")\n\nmount_folder = \"test\"\noutput_folder = \"data\"\ndata_lake_mount_point = \"/mnt/lake\"\nstorage_mount_point = \"/mnt/blob\"\n\n# Data Lake connectioction information and credentials\ndata_lake_configs = {\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n           \"dfs.adls.oauth2.client.id\": data_lake_app_id,\n           \"dfs.adls.oauth2.credential\": data_lake_app_key,\n           \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com/%s/oauth2/token\" % data_lake_app_tenant}\n\ndbutils.fs.mount(\n  source = \"adl://%s.azuredatalakestore.net/%s\" % (data_lake_account, mount_folder),\n  mount_point = data_lake_mount_point,\n  extra_configs = data_lake_configs)\n\nstorage_configs = {\"fs.azure.account.key.%s.blob.core.windows.net\" % storage_account: storage_account_key}\n\ndbutils.fs.mount(\n  source = \"wasbs://%s@%s.blob.core.windows.net/%s\" % (mount_folder, storage_account, mount_folder),\n  mount_point = storage_mount_point,\n  extra_configs = storage_configs)\n\ndbutils.fs.mounts()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from random import random\nfrom time import time\n\nfrom pyspark.sql.types import *\n\nschema = StructType([\n  StructField(\"First\", IntegerType(), True),\n  StructField(\"Second\", IntegerType(), True),\n  StructField(\"Third\", IntegerType(), True),\n  StructField(\"Body\", BinaryType(), True)\n])\n\nfirst_folder_count = 10\nsecond_folder_count = 10\nthird_folder_count = 10\nrows_per_file = 100\n\nprint('Amount of rows in each file: %d' % rows_per_file)\nprint('Total amount of files: %d' % (first_folder_count * second_folder_count * third_folder_count))\n\nvalues = []\nfor first in range(first_folder_count):\n  for second in range(second_folder_count):\n    for third in range(third_folder_count):\n      for i in range(rows_per_file):\n        body = \"{\\\"id\\\":\\\"sensor-id-%s\\\",\\\"v\\\":%f,\\\"t\\\":%d}\" % (i, random(), time() * 1000)\n        values.append((first, second, third, bytearray(body)))\n\nprint('Values list generated')\n\ndf = spark.createDataFrame(values, schema=schema)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["save_folder = \"%s/%s\" % (storage_mount_point, output_folder)\ndf.write.partitionBy(\"First\", \"Second\", \"Third\").format(\"com.databricks.spark.avro\").save(save_folder)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["save_folder = \"%s/%s\" % (data_lake_mount_point, output_folder)\ndf.write.partitionBy(\"First\", \"Second\", \"Third\").format(\"com.databricks.spark.avro\").save(save_folder)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["dbutils.fs.unmount(data_lake_mount_point)\ndbutils.fs.unmount(storage_mount_point)"],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"GeneratingTestData","notebookId":4030296893987360},"nbformat":4,"nbformat_minor":0}
