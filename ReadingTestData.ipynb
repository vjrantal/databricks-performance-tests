{"cells":[{"cell_type":"code","source":["# Before running, either set the matching secrets (https://docs.azuredatabricks.net/user-guide/secrets/secrets.html)\n# or edit the variables below to contain valid connection details\n\nsecrets_scope = \"KEYS\"\n\ndata_lake_app_id = dbutils.secrets.get(secrets_scope, \"DATA_LAKE_APP_ID\")\ndata_lake_app_key = dbutils.secrets.get(secrets_scope, \"DATA_LAKE_APP_KEY\")\ndata_lake_app_tenant = dbutils.secrets.get(secrets_scope, \"DATA_LAKE_APP_TENANT\")\ndata_lake_account = dbutils.secrets.get(secrets_scope, \"DATA_LAKE_ACCOUNT\")\n\nstorage_account_key = dbutils.secrets.get(secrets_scope, \"STORAGE_ACCOUNT_KEY\")\nstorage_account = dbutils.secrets.get(secrets_scope, \"STORAGE_ACCOUNT\")\n\nmount_folder = \"test\"\noutput_folder = \"data\"\ndata_lake_mount_point = \"/mnt/lake\"\nstorage_mount_point = \"/mnt/blob\"\n\n# Data Lake connectioction information and credentials\ndata_lake_configs = {\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n           \"dfs.adls.oauth2.client.id\": data_lake_app_id,\n           \"dfs.adls.oauth2.credential\": data_lake_app_key,\n           \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com/%s/oauth2/token\" % data_lake_app_tenant}\n\nstorage_configs = {\"fs.azure.account.key.%s.blob.core.windows.net\" % storage_account: storage_account_key}\n\nmounts = dbutils.fs.mounts()\n\nif not any(mount[0] == data_lake_mount_point for mount in mounts):\n  dbutils.fs.mount(\n    source = \"adl://%s.azuredatalakestore.net/%s\" % (data_lake_account, mount_folder),\n    mount_point = data_lake_mount_point,\n    extra_configs = data_lake_configs)\n\nif not any(mount[0] == storage_mount_point for mount in mounts):\n  dbutils.fs.mount(\n    source = \"wasbs://%s@%s.blob.core.windows.net/%s\" % (mount_folder, storage_account, mount_folder),\n    mount_point = storage_mount_point,\n    extra_configs = storage_configs)\n\ndbutils.fs.mounts()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import json\nfrom pyspark.sql.functions import from_json\nfrom pyspark.sql.functions import col\n\n# This is to allow reading avro files without .avro extension\nspark.sparkContext._jsc.hadoopConfiguration().set(\"avro.mapred.ignore.inputs.without.extension\", \"false\")\n\ndef combine_into_csv(root_folder):\n  df = spark.read.format(\"com.databricks.spark.avro\").load(\"%s/*/*/*/*.avro\" % root_folder)\n  df = df.withColumn('body', df['body'].cast('string'))\n  body = df.select('body')\n  json_schema = spark.read.json(body.rdd.map(lambda row: row.body)).schema\n  data = body.withColumn('body', from_json(col('body'), json_schema)).select('body.*')\n  data.coalesce(1).write.save(path=('%s/test.csv' % root_folder), format='csv', mode='append', sep=',')\n  #data.write.save(path=('%s/test.csv' % root_folder), format='csv', mode='append', sep=',')"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["read_folder = \"%s/%s\" % (storage_mount_point, output_folder)\ncombine_into_csv(read_folder)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["read_folder = \"%s/%s\" % (data_lake_mount_point, output_folder)\ncombine_into_csv(read_folder)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["dbutils.fs.unmount(data_lake_mount_point)\ndbutils.fs.unmount(storage_mount_point)"],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"ReadingTestData","notebookId":1178226301122127},"nbformat":4,"nbformat_minor":0}
